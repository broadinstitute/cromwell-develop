{{with $environment := env "ENVIRONMENT"}}
{{with $cromwellSecrets := vault (printf "secret/dsde/gotc/%s/cromwell/secrets" $environment)}}

include required(classpath("application"))

webservice {
  port = 8001
  interface = 0.0.0.0
  instance.name = "reference"
  binding-timeout = 30s
}

akka {
  actor.default-dispatcher.fork-join-executor {
    parallelism-factor = 100.0
    parallelism-max = 64
  }

  #Increased from 20s default to allow large metadata responses
  http.server.request-timeout = 55s
  http.server.idle-timeout = 55s

  # Don't warn in server logs (and thus spam Sentry) if client headers are malformed
  http.server.parsing.illegal-header-warnings = off
}

system {
  # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting
  #abort-jobs-on-terminate = false

  # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,
  # in particular clearing up all queued database writes before letting the JVM shut down.
  # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.
  # FC NOTE: reenable this when we want to actually do graceful shutdowns (not just kill the container)
  graceful-server-shutdown = false

  # Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend
  max-retries = 10

  # If 'true' then when Cromwell starts up, it tries to restart incomplete workflows
  workflow-restart = true

  # Cromwell will cap the number of running workflows at N
  max-concurrent-workflows = 12500

  # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  max-workflow-launch-count = 30

  # Number of seconds between workflow launches
  new-workflow-poll-rate = 10

  # Lowered from the default number of cache read workers (25)
  number-of-cache-read-workers = 10

  # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers
  number-of-workflow-log-copy-workers = 10

  # Default number of cache read workers
  number-of-cache-read-workers = 25

  # Maximum scatter width per scatter node. Cromwell will fail the workflow if the scatter width goes beyond N
  # CJL: 5/24/19: Setting this low in response to https://broadworkbench.atlassian.net/browse/PROD-137.
  max-scatter-width-per-scatter = 35000

  # Total max. jobs that can be created per root workflow. If it goes beyond N, Cromwell will fail the workflow by:
  # - no longer creating new jobs
  # - let the jobs that have already been started finish, and then fail the workflow
  # CJL: 5/24/19: Setting this low as an additional response to https://broadworkbench.atlassian.net/browse/PROD-137.
  total-max-jobs-per-root-workflow = 50000

  io {
    # Throttle for GCS calls.
    # this is our quota on broad-dsde-prod
    number-of-requests = 10240611
    per = 100 seconds

    # Number of times an I/O operation should be attempted before giving up and failing it.
    number-of-attempts = 5

    # Amount of time after which an I/O operation will timeout if no response has been received.
    # Note that a timeout may result in a workflow failure so be careful not to set a timeout too low.
    # Unless you start experiencing timeouts under very heavy load there should be no reason to change the default values.
    timeout {
      default = 3 minutes
      # Copy can be a time consuming operation and its timeout can be set separately.
      copy = 1 hour
    }

    gcs {
      parallelism = 10
    }

    nio {
      parallellism = 10
    }
  }

  job-rate-control {
    jobs = 10
    per = 1 second
  }

  # More info: https://docs.google.com/presentation/d/14VgNBDE8Don_oeOOPIWtJQC5OUrcpk3Vip5AQBBCSs4/edit
  hog-safety {
    # Empirically, we have seen that PAPIv2 starts having issues if we submit more jobs than GCS can start for a project.
    # If we do, GCS starts returning 429s which - if they all come at the same time - break PAPI.
    #
    # Since we do not yet make use of private networks, or support hosts without public IPs, the number of jobs we can safely start in PAPI is around 2400 per project.
    #
    # Based on empirical measures of Cromwell's throughput, each Cromwell in the three-instance Horicromtal cluster
    # sets its "concurrent-job-limit = 14400" for a total of 43,200 jobs across the cluster.
    #
    # Based on the formula HogLimit(2400) = TotalJobs(3 * 14400) / HogFactor, we can calculate an instance HogFactor of 18
    hog-factor = 18

    # This is the name of the workflow option that will be sent via workflow the options file and used for hog factor
    #calculations.
    workflow-option = "google_project"

    # Interval between token logs in seconds.
    token-log-interval-seconds = 300
  }

  # Cache file hashes within the scope of a root workflow to prevent repeatedly requesting the
  # hashes of the same files multiple times.
  file-hash-cache = true
}

workflow-options {
  # These workflow options will be encrypted when stored in the database
  encrypted-fields: ["user_service_account_json"]

  # AES-256 key to use to encrypt the values in `encrypted-fields`
  base64-encryption-key: "{{$cromwellSecrets.Data.workflow_options_encryption_key}}"
}

// Optional call-caching configuration.
call-caching {
  # Allows re-use of existing results for jobs you've already run
  # (default: false)
  enabled = true

  # Whether to invalidate a cache result forever if we cannot reuse them. Disable this if you expect some cache copies
  # to fail for external reasons which should not invalidate the cache (e.g. auth differences between users):
  # (default: true)
  invalidate-bad-cache-results = false

  # Filter call cache hits based on authorization failures copying previous call cache hits.
  # May need to be adjusted in the future.
  blacklist-cache {
    enabled = true
    ttl = "1 days"
    size = 5000
    concurrency = 1000
  }
}

drs {
  localization {
    # The Docker image that contains the Martha URL-resolving and localizing code.
    docker-image = "broadinstitute/cromwell-drs-localizer:45-f666098"
  }
}

# This overrides the Martha URL in reference.conf.
filesystems.drs.global.config.martha.url = "https://us-central1-broad-dsde-{{$environment}}.cloudfunctions.net/martha_v2"

google {

  application-name = "cromwell"

  auths = [
    {
      name = "service-account"
      scheme = "service_account"
      json-file = "/etc/cromwell-account.json"
    }
  ]
}

docker {
  hash-lookup {
    // /!\ Attention /!\
    // If you disable this call caching will be disabled for jobs with floating docker tags !
    enabled = true
    // Set this to match your available quota against the Google Container Engine API
    gcr-api-queries-per-100-seconds = 1000
    // Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again
    cache-entry-ttl = "20 minutes"
    // Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache
    cache-size = 200
    // How should docker hashes be looked up. Possible values are "local" and "remote"
    // "local": Lookup hashes on the local docker daemon using the cli
    // "remote": Lookup hashes on docker hub and gcr
    method = "remote"
  }
}

engine {
  # This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.
  # For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.
  # If you intend to be able to run workflows with this kind of declarations:
  # workflow {
  #    String str = read_string("gs://bucket/my-file.txt")
  # }
  # You will need to provide the engine with a gcs filesystem
  # Note that the default filesystem (local) is always available.
  filesystems {
    gcs {
      auth = "service-account"
    },
    drs {
      auth = "service-account"
    },
    local {
      enabled: true
    },
    http {
      enabled: true
    },
  }
}

languages {
  WDL {
    versions {
      "draft-2" {
        language-factory = "languages.wdl.draft2.WdlDraft2LanguageFactory"
      }
      "draft-3" {
        language-factory = "languages.wdl.draft3.WdlDraft3LanguageFactory"
      }
    }
  }
 }

backend {
  default = "PAPIv2"
  providers {
    PAPIv2 {
      actor-factory = "cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory"
      config {
        slow-job-warning-time: 24 hours

        // Google project
        project = {{ if eq $env "prod" }}"broad-gotc-{{$env}}"{{else}}"broad-exomes-dev1"{{end}}

        // Base bucket for workflow executions
        root = "gs://broad-gotc-{{$env}}-cromwell-execution"

        # Set this to the lower of the two values "Queries per 100 seconds" and "Queries per 100 seconds per user" for
        # your project.
        #
        # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will
        # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.
        # 1000 is the default "Queries per 100 seconds per user", 50000 is the default "Queries per 100 seconds"
        # See https://cloud.google.com/genomics/quotas for more information
        genomics-api-queries-per-100-seconds = 50000

        # Polling for completion backs-off gradually for slower-running jobs.
        # This is the maximum polling interval (in seconds):
        maximum-polling-interval = 600

        # This limit is based on empirical measures of Cromwell's throughput. Identical to FireCloud as of 2019-09-04
        concurrent-job-limit = 14400

        # In FireCloud private Docker Hub configuration values will be provided in workflow options only.
        dockerhub { }

        genomics {
          # A reference to an auth defined in the `google` stanza at the top.  This auth is used to create
          # Pipelines and manipulate auth JSONs.
          auth = "service-account"

          // alternative service account to use on the launched compute instance
          // NOTE: If combined with service account authorization, both that serivce account and this service account
          // must be able to read and write to the 'root' GCS path
          compute-service-account = "default"

          # Endpoint for APIs, no reason to change this unless directed by Google.
          endpoint-url = "https://genomics.googleapis.com/"

          # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service
          # account not owned by the submitting user
          restrict-metadata-access = false
        }
        batch-requests {
          timeouts {
            read = 120 seconds
            connect = 120 seconds
          }
        }
        filesystems {
          gcs {
            # A reference to a potentially different auth for manipulating files via engine functions.
            auth = "service-account"
          }
          drs {
            auth = "service-account"
          }
          http {}
        }
      }
    }
  }
}

services {
  MetadataService {
    config {
      metadata-summary-refresh-limit = 50000
    }
  }
  HealthMonitor {
    config {
      check-engine-database: true
      google-auth-name = service-account

      # Check dockerhub, gcs and PAPI for backends
      check-dockerhub: true
      check-gcs: true
      check-papi-backends: [PAPIv2]
      gcs-bucket-to-check = "cromwell-ping-me-{{ or (env "BUCKET_TAG") (env "ENVIRONMENT") }}" #this has to be per env
    }
  }
}

database {
  profile = "slick.jdbc.MySQLProfile$"
  db {
    driver = "com.mysql.cj.jdbc.Driver"
    url = "jdbc:mysql://{{$cromwellSecrets.Data.db_host}}:3306/{{$cromwellSecrets.Data.db_database}}?requireSSL=true&useSSL=true&rewriteBatchedStatements=true"
    user = "{{$cromwellSecrets.Data.db_user}}"
    password = "{{$cromwellSecrets.Data.db_password}}"
    connectionTimeout = 60000
    numThreads = 200
    minThreads = 200
    maxThreads = 200
    minConnections = 200
    maxConnections = 200
    queueSize = 2000
  }
}

# Configuration for load-control related values
load-control {
  ## Queue Size Thresholds ##
  # Cromwell centralizes some operations through singleton actors (possibly acting as routers).
  # This allows for a more efficient control, throttling, and potentially batching of those operations which overall improves throughput.
  # In order to do that, those operations are queued until they can be performed.
  # Those queues are for the most part unbounded in size, which can become a problem under heavy load.
  # Each actor can however let the load controller service know when it considers its work load to be abnormally high.
  # In the case of those queuing actors, this means that their queue size is over a certain threshold.
  # This section allows to configure those threshold values.
  # They should be kept at a reasonable number where reasonable will depend on your system and how much load Cromwell is submitted to.
  # If they're too high they could end up using a lot of memory, if they're too small any small spike will be considered a high load and the system will automatically slow itself down.
  # Benchmarking is recommended to find the values that suit your use case best.
  # If you use the statsD instrumentation service, the queue size and throughput of these actors are instrumented and looking at their value over time can also help you find the right configuration.

  # FC NOTE: this is only applicable for workflow restart (which only occurs on cromwell restart)
  #          we should track this when we restart cromwell (monitor queue size and throughput)
  job-store-read = 10000
  # FC NOTE: this occurs when we workflow jobs are running.  we should monitor queue size and throughput
  #          while cromwell is running to determine if we need to change this value
  job-store-write = 10000


  # call cache read actors are routed (several actors are performing cache read operations
  # this threshold applies to each routee individually, so set it to a lower value to account for that
  # to change the number of routees, see the services.number-of-cache-read-workers config value
  # FC NOTE: monitor queue size and throughput when we are seeing jobs that are slowed down by
  #          call cache lookup step
  call-cache-read = 1000
  call-cache-write = 10000

  # FC NOTE: this is storing of the papi job id only at the moment
  key-value-read = 10000
  key-value-write = 10000


  # The I/O queue has the specificity to be bounded. This sets its size
  # FC NOTE: io operations directly to google buckets (e.g. exec.sh)
  io-queue-size = 10000
  # If the I/O queue is full, subsequent requests are rejected and to be retried later.
  # This time window specifies how much time without request rejection consititutes a "back to normal load" event
  io-normal-window = 10s

  # FC NOTE: we should closely watch this one since it's a known bottleneck for FC
  # metadata is an order of magnitude higher because its normal load is much higher than other actors
  metadata-write = 100000

  ## Backend specific ##
  # Google requests to the Pipelines API are also queued and batched
  papi-requests = 10000

  ## Misc. ##
  # How often each actor should update its perceived load
  monitoring-frequency = 5 seconds

  # FC NOTE: at the moment memory management below isn't really working right anyway,
  #          but doesn't cause harm. leave alone for now
  # Memory is monitored by looking at the amount of free memory left
  memory-threshold-in-mb = 1024
  # Because memory measurements are not precise, the last memory-measurement-window measurements are recorded
  # and changes on load are only triggered if all measurements are below or above the threshold
  # The default value 6 combined with a monitoring frequencey of 5 seconds means that load alerts are triggered
  # If the memory is below the threshold for at least 30 consecutive seconds
  memory-measurement-window = 6
}
{{end}}{{end}}
