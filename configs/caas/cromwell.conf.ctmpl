{{with $environment := env "ENVIRONMENT"}}
{{with $cromwellSecrets := vault (printf "secret/dsde/caas/%s/cromwell/secrets" $environment)}}
{{with $commonSecrets := vault (printf "secret/dsde/caas/%s/common/secrets" $environment)}}

include "application.conf"

webservice {
  port = 8001
  interface = 0.0.0.0
  instance.name = "reference"
}

akka {
  actor.default-dispatcher.fork-join-executor {
    # Number of threads = min(parallelism-factor * cpus, parallelism-max)
    # Below are the default values set by Akka, uncomment to tune these

    parallelism-factor = 100.0
    parallelism-max = 64
  }

  actor.guardian-supervisor-strategy = "cromwell.core.CromwellUserGuardianStrategy"

  priority-mailbox {
    mailbox-type = "akka.dispatch.UnboundedControlAwareMailbox"
  }

  dispatchers {
    # A dispatcher for actors performing blocking io operations
    # Prevents the whole system from being slowed down when waiting for responses from external resources for instance
    io-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      # Using the forkjoin defaults, this can be tuned if we wish
    }

    # A dispatcher for actors handling API operations
    # Keeps the API responsive regardless of the load of workflows being run
    api-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher for engine actors
    # Because backends behavior is unpredictable (potentially blocking, slow) the engine runs
    # on its own dispatcher to prevent backends from affecting its performance.
    engine-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher used by supported backend actors
    backend-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher used for the service registry
    service-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher to bulkhead the health monitor from the rest of the system. Sets throughput low in order to
    # ensure the monitor is fairly low priority
    health-monitor-dispatcher {
      type = Dispatcher
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 4
      }

      throughput = 1
    }
    # Note that without further configuration, all other actors run on the default dispatcher
  }

  coordinated-shutdown.phases {
    abort-all-workflows {
      # This phase is used to give time to Cromwell to abort all workflows upon shutdown.
      # It's only used if system.abort-jobs-on-terminate = true
      # This timeout can be adjusted to give more or less time to Cromwell to abort workflows
      timeout = 1 hour
      depends-on = [service-unbind]
    }

    stop-io-activity{
      # Adjust this timeout according to the maximum amount of time Cromwell
      # should be allowed to spend flushing its database queues
      timeout = 30 minutes
      depends-on = [service-stop]
    }
  }

  #Increased from 20s default to allow large metadata responses
  http.server.request-timeout = 55s
  http.server.idle-timeout = 55s
}

system {
  # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting
  #abort-jobs-on-terminate = false

  # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,
  # in particular clearing up all queued database writes before letting the JVM shut down.
  # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.
  # FC NOTE: reenable this when we want to actually do graceful shutdowns (not just kill the container)
  graceful-server-shutdown = false

  # Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend
  max-retries = 10

  # If 'true' then when Cromwell starts up, it tries to restart incomplete workflows
  workflow-restart = true

  # Cromwell will cap the number of running workflows at N
  max-concurrent-workflows = 25000

  # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  max-workflow-launch-count = 50

  # Number of seconds between workflow launches
  new-workflow-poll-rate = 20

  # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers
  number-of-workflow-log-copy-workers = 10

  # Default number of cache read workers
  number-of-cache-read-workers = 25

  # Maximum scatter width per scatter node. Cromwell will fail the workflow if the scatter width goes beyond N
  # CJL: 5/24/19: Setting this low in response to https://broadworkbench.atlassian.net/browse/PROD-137.
  max-scatter-width-per-scatter = 75000

  # Total max. jobs that can be created per root workflow. If it goes beyond N, Cromwell will fail the workflow by:
  # - no longer creating new jobs
  # - let the jobs that have already been started finish, and then fail the workflow
  # CJL: 5/24/19: Setting this low as an additional response to https://broadworkbench.atlassian.net/browse/PROD-137.
  total-max-jobs-per-root-workflow = 200000

  io {
    # Throttle for GCS calls.
    # this is our quota on broad-dsde-prod
    number-of-requests = 10240611
    per = 100 seconds

    # Number of times an I/O operation should be attempted before giving up and failing it.
    number-of-attempts = 5

    # Amount of time after which an I/O operation will timeout if no response has been received.
    # Note that a timeout may result in a workflow failure so be careful not to set a timeout too low.
    # Unless you start experiencing timeouts under very heavy load there should be no reason to change the default values.
    timeout {
      default = 3 minutes
      # Copy can be a time consuming operation and its timeout can be set separately.
      copy = 1 hour
    }

    gcs {
      parallelism = 10
    }

    nio {
      parallellism = 10
    }
  }

  # Rate at which Cromwell updates it's instrumentation gauge metrics (e.g: Number of workflows running, queued, etc..)
  instrumentation-rate = 5 seconds

  job-rate-control {
    jobs = 50
    per = 1 second
  }

  # More info: https://docs.google.com/presentation/d/14VgNBDE8Don_oeOOPIWtJQC5OUrcpk3Vip5AQBBCSs4/edit
  hog-safety {
    # 2400 jobs per project in PAPI
    # Empirically, a Cromwell of this size (8 vCPU/30 GB) can safely run 14400 jobs
    # Based on the calculations in the presentation above: 14400 / 2400 = 6
    # If you know any additional information about the IP limit, etc., please update this comment.
    hog-factor = 6

    # This is the name of the workflow option that will be sent via workflow the options file and used for hog factor
    #calculations.
    workflow-option = "google_project"

    # Interval between token logs in seconds.
    token-log-interval-seconds = 300
  }

  # Cache file hashes within the scope of a root workflow to prevent repeatedly requesting the
  # hashes of the same files multiple times.
  file-hash-cache = true
}

workflow-options {
  # These workflow options will be encrypted when stored in the database
  encrypted-fields: ["user_service_account_json"]

  # AES-256 key to use to encrypt the values in `encrypted-fields`
  base64-encryption-key: "{{$cromwellSecrets.Data.workflow_options_encryption_key}}"

  # Directory where to write per workflow logs
  workflow-log-dir: "cromwell-workflow-logs"

  # When true, per workflow logs will be deleted after copying
  workflow-log-temporary: true

  # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.
  # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:
  #workflow-failure-mode: "ContinueWhilePossible"
}

// Optional call-caching configuration.
call-caching {
  # Allows re-use of existing results for jobs you've already run
  # (default: false)
  enabled = true

  # Whether to invalidate a cache result forever if we cannot reuse them. Disable this if you expect some cache copies
  # to fail for external reasons which should not invalidate the cache (e.g. auth differences between users):
  # (default: true)
  invalidate-bad-cache-results = false

  # Filter call cache hits based on authorization failures copying previous call cache hits.
  # May need to be adjusted in the future.
  blacklist-cache {
    enabled = true
    ttl = "1 days"
    size = 5000
    concurrency = 1000
  }
}

drs {
    localization {
        # The Docker image that contains the Martha URL-resolving and localizing code.
        docker-image = "broadinstitute/cromwell-drs-localizer:45-d46ff9f"
    }
}

# This overrides the Martha URL in reference.conf.
filesystems.drs.global.config.martha.url = "https://us-central1-broad-dsde-{{$environment}}.cloudfunctions.net/martha_v2"

google {

  application-name = "cromwell"

  auths = [
    {
      name = "user-service-account"
      scheme = "user_service_account"
    }
  ]
}

docker {
  hash-lookup {
    // /!\ Attention /!\
    // If you disable this call caching will be disabled for jobs with floating docker tags !
    enabled = true
    // Set this to match your available quota against the Google Container Engine API
    gcr-api-queries-per-100-seconds = 1000
    // Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again
    cache-entry-ttl = "20 minutes"
    // Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache
    cache-size = 200
    // How should docker hashes be looked up. Possible values are "local" and "remote"
    // "local": Lookup hashes on the local docker daemon using the cli
    // "remote": Lookup hashes on docker hub and gcr
    method = "remote"
  }
}

engine {
  # This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.
  # For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.
  # If you intend to be able to run workflows with this kind of declarations:
  # workflow {
  #    String str = read_string("gs://bucket/my-file.txt")
  # }
  # You will need to provide the engine with a gcs filesystem
  # Note that the default filesystem (local) is always available.
  filesystems {
    gcs {
      auth = "user-service-account"
    },
    drs {
      auth = "user-service-account"
    },
    local {
      enabled: false
    }
  }
}

languages {
  WDL {
    versions {
      "draft-2" {
        language-factory = "languages.wdl.draft2.WdlDraft2LanguageFactory"
      }
      "draft-3" {
        language-factory = "languages.wdl.draft3.WdlDraft3LanguageFactory"
      }
    }
  }
#   CWL {
#     versions {
#       "v1.0" {
#         language-factory = "languages.cwl.CwlV1_0LanguageFactory"
#       }
#     }
#   }
 }

backend {
  default = "JES"
  providers {
    Local {
      # setting config.root to /dev/null effectivly disables the local backend preventing end users from running code on the cromwell VM
      config.root = "/dev/null"
    }

    JES {
      actor-factory = "cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory"
      config {
        slow-job-warning-time: 24 hours

        // Google project
        project = "user_error: google_project must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"

        // Base bucket for workflow executions
        root = "user_error: jes_gcs_root must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"

        # Set this to the lower of the two values "Queries per 100 seconds" and "Queries per 100 seconds per user" for
        # your project.
        #
        # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will
        # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.
        # 1000 is the default "Queries per 100 seconds per user", 50000 is the default "Queries per 100 seconds"
        # See https://cloud.google.com/genomics/quotas for more information
        genomics-api-queries-per-100-seconds = 50000

        # Polling for completion backs-off gradually for slower-running jobs.
        # This is the maximum polling interval (in seconds):
        maximum-polling-interval = 600

        # Optional Dockerhub Credentials. Can be used to access private docker images.
        dockerhub {
          account = "dockerhub@broadinstitute.org"
          token = "{{$commonSecrets.Data.docker_token}}"
        }

        genomics {
          # A reference to an auth defined in the `google` stanza at the top.  This auth is used to create
          # Pipelines and manipulate auth JSONs.
          auth = "user-service-account"

          // alternative service account to use on the launched compute instance
          // NOTE: If combined with service account authorization, both that serivce account and this service account
          // must be able to read and write to the 'root' GCS path
          compute-service-account = "default"

          # Endpoint for APIs, no reason to change this unless directed by Google.
          endpoint-url = "https://genomics.googleapis.com/"
        }

        filesystems {
          gcs {
            # A reference to a potentially different auth for manipulating files via engine functions.
            auth = "user-service-account"
          }
	  http {}
        }
      }
    }

    PAPIv2 {
      actor-factory = "cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory"
      config {
        slow-job-warning-time: 24 hours

        // Google project
        project = "user_error: google_project must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"

        // Base bucket for workflow executions
        root = "user_error: jes_gcs_root must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"

        # Set this to the lower of the two values "Queries per 100 seconds" and "Queries per 100 seconds per user" for
        # your project.
        #
        # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will
        # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.
        # 1000 is the default "Queries per 100 seconds per user", 50000 is the default "Queries per 100 seconds"
        # See https://cloud.google.com/genomics/quotas for more information
        genomics-api-queries-per-100-seconds = 50000

        # Polling for completion backs-off gradually for slower-running jobs.
        # This is the maximum polling interval (in seconds):
        maximum-polling-interval = 600

        # This limit is based on empirical measures of Cromwell's throughput. Identical to FireCloud as of 2019-08
        concurrent-job-limit = 14400

        # In FireCloud private Docker Hub configuration values will be provided in workflow options only.
        dockerhub { }

        genomics {
          # A reference to an auth defined in the `google` stanza at the top.  This auth is used to create
          # Pipelines and manipulate auth JSONs.
          auth = "user-service-account"

          // alternative service account to use on the launched compute instance
          // NOTE: If combined with service account authorization, both that serivce account and this service account
          // must be able to read and write to the 'root' GCS path
          compute-service-account = "default"

          # Endpoint for APIs, no reason to change this unless directed by Google.
          endpoint-url = "https://genomics.googleapis.com/"

          # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service
          # account not owned by the submitting user
          restrict-metadata-access = false
        }
        batch-requests {
          timeouts {
            read = 120 seconds
            connect = 120 seconds
          }
        }
        filesystems {
          gcs {
            # A reference to a potentially different auth for manipulating files via engine functions.
            auth = "user-service-account"
          }
          drs {
            auth = "user-service-account"
          }
          http {}
        }
      }
    }
  }
}

services {
  KeyValue {
    class = "cromwell.services.keyvalue.impl.SqlKeyValueServiceActor"
  }
  MetadataService {
    class = "cromwell.services.metadata.impl.MetadataServiceActor"
    config {
      # Set this value to "Inf" to turn off metadata summary refresh.  The default value is currently "2 seconds".
      # metadata-summary-refresh-interval = "Inf"


      metadata-summary-refresh-limit = 50000
    }
  }
  Instrumentation {
     # Default noop service - instrumentation metrics are ignored
     class = "cromwell.services.instrumentation.impl.noop.NoopInstrumentationServiceActor"
  }
  LoadController {
    class = "cromwell.services.loadcontroller.impl.LoadControllerServiceActor"
    config {
      # The load controller service will periodically look at the status of various metrics its collecting and make an
      # assessment of the system's load. If necessary an alert will be sent to the rest of the system.
      # This option sets how frequently this should happen
      control-frequency = 5 seconds
    }
  }
}

database {
  profile = "slick.jdbc.MySQLProfile$"
  db {
    driver = "com.mysql.jdbc.Driver"
    # driver = "com.mysql.cj.jdbc.Driver" <-- Switching *after* the PROD instance upgrades to 38. Til then will generate log warnings.
    url = "jdbc:mysql://cromwell-mysql.caas-{{$environment}}.broadinstitute.org:3306/cromwell?requireSSL=true&useSSL=true&rewriteBatchedStatements=true"
    user = "{{$cromwellSecrets.Data.db_user}}"
    password = "{{$cromwellSecrets.Data.db_password}}"
    connectionTimeout = 60000
    numThreads = 200
  }

  migration {
    # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of
    # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be
    # retrieved and processed at a time, before asking for the next chunk.
    read-batch-size = 100000

    # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single
    # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out
    # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.
    write-batch-size = 100000
  }
}

# Configuration for load-control related values
load-control {
  ## Queue Size Thresholds ##
  # Cromwell centralizes some operations through singleton actors (possibly acting as routers).
  # This allows for a more efficient control, throttling, and potentially batching of those operations which overall improves throughput.
  # In order to do that, those operations are queued until they can be performed.
  # Those queues are for the most part unbounded in size, which can become a problem under heavy load.
  # Each actor can however let the load controller service know when it considers its work load to be abnormally high.
  # In the case of those queuing actors, this means that their queue size is over a certain threshold.
  # This section allows to configure those threshold values.
  # They should be kept at a reasonable number where reasonable will depend on your system and how much load Cromwell is submitted to.
  # If they're too high they could end up using a lot of memory, if they're too small any small spike will be considered a high load and the system will automatically slow itself down.
  # Benchmarking is recommended to find the values that suit your use case best.
  # If you use the statsD instrumentation service, the queue size and throughput of these actors are instrumented and looking at their value over time can also help you find the right configuration.

  # FC NOTE: this is only applicable for workflow restart (which only occurs on cromwell restart)
  #          we should track this when we restart cromwell (monitor queue size and throughput)
  job-store-read = 10000
  # FC NOTE: this occurs when we workflow jobs are running.  we should monitor queue size and throughput
  #          while cromwell is running to determine if we need to change this value
  job-store-write = 10000


  # call cache read actors are routed (several actors are performing cache read operations
  # this threshold applies to each routee individually, so set it to a lower value to account for that
  # to change the number of routees, see the services.number-of-cache-read-workers config value
  # FC NOTE: monitor queue size and throughput when we are seeing jobs that are slowed down by
  #          call cache lookup step
  call-cache-read = 1000
  call-cache-write = 10000

  # FC NOTE: this is storing of the papi job id only at the moment
  key-value-read = 10000
  key-value-write = 10000


  # The I/O queue has the specificity to be bounded. This sets its size
  # FC NOTE: io operations directly to google buckets (e.g. exec.sh)
  io-queue-size = 10000
  # If the I/O queue is full, subsequent requests are rejected and to be retried later.
  # This time window specifies how much time without request rejection consititutes a "back to normal load" event
  io-normal-window = 10s

  # FC NOTE: we should closely watch this one since it's a known bottleneck for FC
  # metadata is an order of magnitude higher because its normal load is much higher than other actors
  metadata-write = 100000

  ## Backend specific ##
  # Google requests to the Pipelines API are also queued and batched
  papi-requests = 10000

  ## Misc. ##
  # How often each actor should update its perceived load
  monitoring-frequency = 5 seconds

  # FC NOTE: at the moment memory management below isn't really working right anyway,
  #          but doesn't cause harm. leave alone for now
  # Memory is monitored by looking at the amount of free memory left
  memory-threshold-in-mb = 1024
  # Because memory measurements are not precise, the last memory-measurement-window measurements are recorded
  # and changes on load are only triggered if all measurements are below or above the threshold
  # The default value 6 combined with a monitoring frequencey of 5 seconds means that load alerts are triggered
  # If the memory is below the threshold for at least 30 consecutive seconds
  memory-measurement-window = 6
}
{{end}}{{end}}{{end}}
